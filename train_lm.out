perplexity at alpha=[0.01, 0.2, 0.79]: 19994.664201
2-gram for ar, word level, data size 66150 sentences
perplexity at alpha=[0.01, 0.2, 0.79]: 15.752286
2-gram for ar, char level, data size 66150 sentences
perplexity at alpha=[0.01, 0.2, 0.79]: 19994.664201
3-gram for ar, word level, data size 66150 sentences
perplexity at alpha=[0.01, 0.2, 0.79]: 15.752286
3-gram for ar, char level, data size 66150 sentences


perplexity at alpha=[0.01, 0.2, 0.79]: 2897.238380
2-gram for de, word level, data size 53703 sentences
perplexity at alpha=[0.01, 0.2, 0.79]: 11.614498
2-gram for de, char level, data size 53703 sentences
perplexity at alpha=[0.01, 0.2, 0.79]: 2897.238380
3-gram for de, word level, data size 53703 sentences
perplexity at alpha=[0.01, 0.2, 0.79]: 11.614498
3-gram for de, char level, data size 53703 sentences


perplexity at alpha=[0.01, 0.2, 0.79]: 756.845576
2-gram for en, word level, data size 104155 sentences
perplexity at alpha=[0.01, 0.2, 0.79]: 11.767659
2-gram for en, char level, data size 104155 sentences
perplexity at alpha=[0.01, 0.2, 0.79]: 756.845576
3-gram for en, word level, data size 104155 sentences
perplexity at alpha=[0.01, 0.2, 0.79]: 11.767659
3-gram for en, char level, data size 104155 sentences


perplexity at alpha=[0.01, 0.2, 0.79]: 1108.775377
2-gram for es, word level, data size 75744 sentences
perplexity at alpha=[0.01, 0.2, 0.79]: 10.969812
2-gram for es, char level, data size 75744 sentences
perplexity at alpha=[0.01, 0.2, 0.79]: 1108.775377
3-gram for es, word level, data size 75744 sentences
perplexity at alpha=[0.01, 0.2, 0.79]: 10.969812
3-gram for es, char level, data size 75744 sentences


perplexity at alpha=[0.01, 0.2, 0.79]: 1955.505066
2-gram for fa, word level, data size 76793 sentences
perplexity at alpha=[0.01, 0.2, 0.79]: 13.658443
2-gram for fa, char level, data size 76793 sentences
perplexity at alpha=[0.01, 0.2, 0.79]: 1955.505066
3-gram for fa, word level, data size 76793 sentences
perplexity at alpha=[0.01, 0.2, 0.79]: 13.658443
3-gram for fa, char level, data size 76793 sentences


perplexity at alpha=[0.01, 0.2, 0.79]: 775.995944
2-gram for fr, word level, data size 128330 sentences
perplexity at alpha=[0.01, 0.2, 0.79]: 11.392539
2-gram for fr, char level, data size 128330 sentences
perplexity at alpha=[0.01, 0.2, 0.79]: 775.995944
3-gram for fr, word level, data size 128330 sentences
perplexity at alpha=[0.01, 0.2, 0.79]: 11.392539
3-gram for fr, char level, data size 128330 sentences


perplexity at alpha=[0.01, 0.2, 0.79]: 3505.174889
2-gram for hi, word level, data size 3850 sentences
perplexity at alpha=[0.01, 0.2, 0.79]: 13.833520
2-gram for hi, char level, data size 3850 sentences
perplexity at alpha=[0.01, 0.2, 0.79]: 3505.174889
3-gram for hi, word level, data size 3850 sentences
perplexity at alpha=[0.01, 0.2, 0.79]: 13.833520
3-gram for hi, char level, data size 3850 sentences


perplexity at alpha=[0.01, 0.2, 0.79]: 8229156714.271397
2-gram for ja, word level, data size 112552 sentences
perplexity at alpha=[0.01, 0.2, 0.79]: 39.752232
2-gram for ja, char level, data size 112552 sentences
perplexity at alpha=[0.01, 0.2, 0.79]: 8229156714.271397
3-gram for ja, word level, data size 112552 sentences
perplexity at alpha=[0.01, 0.2, 0.79]: 39.752232
3-gram for ja, char level, data size 112552 sentences


perplexity at alpha=[0.01, 0.2, 0.79]: 54328.575305
2-gram for ko, word level, data size 108666 sentences
perplexity at alpha=[0.01, 0.2, 0.79]: 24.863288
2-gram for ko, char level, data size 108666 sentences
perplexity at alpha=[0.01, 0.2, 0.79]: 54328.575305
3-gram for ko, word level, data size 108666 sentences
perplexity at alpha=[0.01, 0.2, 0.79]: 24.863288
3-gram for ko, char level, data size 108666 sentences


perplexity at alpha=[0.01, 0.2, 0.79]: 1432.422596
2-gram for nl, word level, data size 72224 sentences
perplexity at alpha=[0.01, 0.2, 0.79]: 11.148851
2-gram for nl, char level, data size 72224 sentences
perplexity at alpha=[0.01, 0.2, 0.79]: 1432.422596
3-gram for nl, word level, data size 72224 sentences
perplexity at alpha=[0.01, 0.2, 0.79]: 11.148851
3-gram for nl, char level, data size 72224 sentences


perplexity at alpha=[0.01, 0.2, 0.79]: 6466.626994
2-gram for ru, word level, data size 117477 sentences
perplexity at alpha=[0.01, 0.2, 0.79]: 14.046007
2-gram for ru, char level, data size 117477 sentences
perplexity at alpha=[0.01, 0.2, 0.79]: 6466.626994
3-gram for ru, word level, data size 117477 sentences
perplexity at alpha=[0.01, 0.2, 0.79]: 14.046007
3-gram for ru, char level, data size 117477 sentences


perplexity at alpha=[0.01, 0.2, 0.79]: 861431.047777
2-gram for ta, word level, data size 2206 sentences
perplexity at alpha=[0.01, 0.2, 0.79]: 10.984176
2-gram for ta, char level, data size 2206 sentences
perplexity at alpha=[0.01, 0.2, 0.79]: 861431.047777
3-gram for ta, word level, data size 2206 sentences
perplexity at alpha=[0.01, 0.2, 0.79]: 10.984176
3-gram for ta, char level, data size 2206 sentences


